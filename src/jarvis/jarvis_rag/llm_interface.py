from abc import ABC, abstractmethod
import os
import os
from abc import ABC, abstractmethod

from jarvis.jarvis_agent import Agent as JarvisAgent
from jarvis.jarvis_platform.base import BasePlatform
from jarvis.jarvis_platform.registry import PlatformRegistry


class LLMInterface(ABC):
    """
    Abstract Base Class for Large Language Model interfaces.

    This class defines the standard interface for interacting with a remote LLM.
    Any LLM provider (OpenAI, Anthropic, etc.) should be implemented as a
    subclass of this interface.
    """

    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> str:
        """
        Generates a response from the LLM based on a given prompt.

        Args:
            prompt: The input prompt to send to the LLM.
            **kwargs: Additional keyword arguments for the LLM API call
                      (e.g., temperature, max_tokens).

        Returns:
            The text response generated by the LLM.
        """
        pass


class OpenAI_LLM(LLMInterface):
    """
    An implementation of the LLMInterface for OpenAI's models (GPT series).

    This class uses the 'openai' Python client to interact with the API.
    It requires the OPENAI_API_KEY environment variable to be set.
    """

    def __init__(self, model_name: str = "gpt-4o"):
        """
        Initializes the OpenAI LLM client.

        Args:
            model_name: The name of the OpenAI model to use (e.g., "gpt-4o", "gpt-3.5-turbo").
        """
        try:
            from openai import OpenAI
        except ImportError:
            raise ImportError(
                "OpenAI client not found. Please install it with 'pip install openai'."
            )

        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY environment variable not set.")

        self.client = OpenAI(api_key=self.api_key)
        self.model_name = model_name
        print(f"🚀 已初始化 OpenAI LLM，模型: {self.model_name}")

    def generate(self, prompt: str, **kwargs) -> str:
        """
        Sends a prompt to the OpenAI API and returns the response.

        Args:
            prompt: The user's prompt.
            **kwargs: Supports standard OpenAI API parameters like 'temperature',
                      'max_tokens', etc.

        Returns:
            The content of the first choice from the chat completion.
        """
        # Set default parameters if not provided
        params = {
            "temperature": 0.7,
            "max_tokens": 1500,
            **kwargs,
        }

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                **params,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"❌ 调用 OpenAI API 时发生错误: {e}")
            return "错误: 无法从LLM获取响应。"


class ToolAgent_LLM(LLMInterface):
    """
    An implementation of the LLMInterface that uses a tool-wielding JarvisAgent
    to generate the final response.
    """

    def __init__(self):
        """
        Initializes the Tool-Agent LLM wrapper.
        """
        print("🤖 已初始化工具 Agent 作为最终应答者。")
        self.allowed_tools = ["read_code", "execute_script"]
        # A generic system prompt for the agent
        self.system_prompt = "You are a helpful assistant. Please answer the user's question based on the provided context. You can use tools to find more information if needed."
        self.summary_prompt = """
<report>
请为本次问答任务生成一个总结报告，包含以下内容：

1. **原始问题**: 重述用户最开始提出的问题。
2. **关键信息来源**: 总结你是基于哪些关键信息或文件得出的结论。
3. **最终答案**: 给出最终的、精炼的回答。
</report>
"""

    def generate(self, prompt: str, **kwargs) -> str:
        """
        Runs the JarvisAgent with a restricted toolset to generate an answer.

        Args:
            prompt: The full prompt, including context, to be sent to the agent.
            **kwargs: Ignored, kept for interface compatibility.

        Returns:
            The final answer generated by the agent.
        """
        try:
            # Initialize the agent with specific settings for RAG context
            agent = JarvisAgent(
                system_prompt=self.system_prompt,
                use_tools=self.allowed_tools,
                auto_complete=True,
                use_methodology=False,
                use_analysis=False,
                need_summary=True,
                summary_prompt=self.summary_prompt,
            )

            # The agent's run method expects the 'user_input' parameter
            final_answer = agent.run(user_input=prompt)
            return str(final_answer)

        except Exception as e:
            print(f"❌ Agent 在执行过程中发生错误: {e}")
            return "错误: Agent 未能成功生成回答。"


class JarvisPlatform_LLM(LLMInterface):
    """
    An implementation of the LLMInterface for the project's internal platform.

    This class uses the PlatformRegistry to get the configured "normal" model.
    """

    def __init__(self):
        """
        Initializes the Jarvis Platform LLM client.
        """
        try:
            self.registry = PlatformRegistry.get_global_platform_registry()
            self.platform: BasePlatform = self.registry.get_normal_platform()
            self.platform.set_suppress_output(
                False
            )  # Ensure no console output from the model
            print(f"🚀 已初始化 Jarvis 平台 LLM，模型: {self.platform.name()}")
        except Exception as e:
            print(f"❌ 初始化 Jarvis 平台 LLM 失败: {e}")
            raise

    def generate(self, prompt: str, **kwargs) -> str:
        """
        Sends a prompt to the local platform model and returns the response.

        Args:
            prompt: The user's prompt.
            **kwargs: Ignored, kept for interface compatibility.

        Returns:
            The response generated by the platform model.
        """
        try:
            # Use the robust chat_until_success method
            return self.platform.chat_until_success(prompt)
        except Exception as e:
            print(f"❌ 调用 Jarvis 平台模型时发生错误: {e}")
            return "错误: 无法从本地LLM获取响应。"
