from abc import ABC, abstractmethod
import os
from jarvis.jarvis_platform.registry import PlatformRegistry
from jarvis.jarvis_platform.base import BasePlatform


class LLMInterface(ABC):
    """
    Abstract Base Class for Large Language Model interfaces.

    This class defines the standard interface for interacting with a remote LLM.
    Any LLM provider (OpenAI, Anthropic, etc.) should be implemented as a
    subclass of this interface.
    """

    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> str:
        """
        Generates a response from the LLM based on a given prompt.

        Args:
            prompt: The input prompt to send to the LLM.
            **kwargs: Additional keyword arguments for the LLM API call
                      (e.g., temperature, max_tokens).

        Returns:
            The text response generated by the LLM.
        """
        pass


class OpenAI_LLM(LLMInterface):
    """
    An implementation of the LLMInterface for OpenAI's models (GPT series).

    This class uses the 'openai' Python client to interact with the API.
    It requires the OPENAI_API_KEY environment variable to be set.
    """

    def __init__(self, model_name: str = "gpt-4o"):
        """
        Initializes the OpenAI LLM client.

        Args:
            model_name: The name of the OpenAI model to use (e.g., "gpt-4o", "gpt-3.5-turbo").
        """
        try:
            from openai import OpenAI
        except ImportError:
            raise ImportError(
                "OpenAI client not found. Please install it with 'pip install openai'."
            )

        self.api_key = os.getenv("OPENAI_API_KEY")
        if not self.api_key:
            raise ValueError("OPENAI_API_KEY environment variable not set.")

        self.client = OpenAI(api_key=self.api_key)
        self.model_name = model_name
        print(f"Initialized OpenAI LLM with model: {self.model_name}")

    def generate(self, prompt: str, **kwargs) -> str:
        """
        Sends a prompt to the OpenAI API and returns the response.

        Args:
            prompt: The user's prompt.
            **kwargs: Supports standard OpenAI API parameters like 'temperature',
                      'max_tokens', etc.

        Returns:
            The content of the first choice from the chat completion.
        """
        # Set default parameters if not provided
        params = {
            "temperature": 0.7,
            "max_tokens": 1500,
            **kwargs,
        }

        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "system", "content": "You are a helpful assistant."},
                    {"role": "user", "content": prompt},
                ],
                **params,
            )
            return response.choices[0].message.content.strip()
        except Exception as e:
            print(f"An error occurred while calling the OpenAI API: {e}")
            return "Error: Could not get a response from the LLM."


class JarvisPlatform_LLM(LLMInterface):
    """
    An implementation of the LLMInterface for the project's internal platform.

    This class uses the PlatformRegistry to get the configured "thinking" model.
    """

    def __init__(self):
        """
        Initializes the Jarvis Platform LLM client.
        """
        try:
            self.registry = PlatformRegistry.get_global_platform_registry()
            self.platform: BasePlatform = self.registry.get_thinking_platform()
            self.platform.set_suppress_output(
                False
            )  # Ensure no console output from the model
            print(f"Initialized Jarvis Platform LLM with model: {self.platform.name()}")
        except Exception as e:
            print(f"Failed to initialize Jarvis Platform LLM: {e}")
            raise

    def generate(self, prompt: str, **kwargs) -> str:
        """
        Sends a prompt to the local platform model and returns the response.

        Args:
            prompt: The user's prompt.
            **kwargs: Ignored, kept for interface compatibility.

        Returns:
            The response generated by the platform model.
        """
        try:
            # Use the robust chat_until_success method
            return self.platform.chat_until_success(prompt)
        except Exception as e:
            print(f"An error occurred while calling the Jarvis Platform model: {e}")
            return "Error: Could not get a response from the local LLM."
