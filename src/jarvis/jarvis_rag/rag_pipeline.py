import os
from typing import List, Literal, Optional

from langchain.docstore.document import Document

from .embedding_manager import EmbeddingManager
from .llm_interface import LLMInterface, JarvisPlatform_LLM
from .retriever import ChromaRetriever
from jarvis.jarvis_utils.config import (
    get_rag_embedding_mode,
    get_rag_vector_db_path,
    get_rag_embedding_cache_path,
    get_rag_embedding_models,
)


class JarvisRAGPipeline:
    """
    The main orchestrator for the RAG pipeline.

    This class integrates the embedding manager, retriever, and LLM to provide
    a complete pipeline for adding documents and querying them.
    """

    def __init__(
        self,
        llm: Optional[LLMInterface] = None,
        embedding_mode: Optional[Literal["performance", "accuracy"]] = None,
        db_path: Optional[str] = None,
        collection_name: str = "jarvis_rag_collection",
    ):
        """
        Initializes the RAG pipeline.

        Args:
            llm: An instance of a class implementing LLMInterface.
                 If None, defaults to OpenAI_LLM().
            embedding_mode: The mode for the local embedding model. If None, uses config value.
            db_path: Path to the persistent vector database. If None, uses config value.
            collection_name: Name of the collection in the vector database.
        """
        # Determine the embedding model to isolate data paths
        _embedding_mode = embedding_mode or get_rag_embedding_mode()
        embedding_models = get_rag_embedding_models()
        model_name = embedding_models[_embedding_mode]["model_name"]
        sanitized_model_name = model_name.replace("/", "_").replace("\\", "_")

        # If a specific db_path is given, use it. Otherwise, create a model-specific path.
        _final_db_path = (
            str(db_path)
            if db_path
            else os.path.join(get_rag_vector_db_path(), sanitized_model_name)
        )
        # Always create a model-specific cache path.
        _final_cache_path = os.path.join(
            get_rag_embedding_cache_path(), sanitized_model_name
        )

        self.embedding_manager = EmbeddingManager(
            mode=_embedding_mode, cache_dir=_final_cache_path
        )
        self.retriever = ChromaRetriever(
            embedding_manager=self.embedding_manager,
            db_path=_final_db_path,
            collection_name=collection_name,
        )
        # Default to the local Jarvis Platform LLM
        self.llm = llm if llm is not None else JarvisPlatform_LLM()

        print("JarvisRAGPipeline initialized successfully.")

    def add_documents(self, documents: List[Document]):
        """
        Adds documents to the vector knowledge base.

        Args:
            documents: A list of LangChain Document objects to add.
        """
        self.retriever.add_documents(documents)

    def _create_prompt(self, query: str, context_docs: List[Document]) -> str:
        """Creates the final prompt for the LLM."""
        context = "\n\n".join([doc.page_content for doc in context_docs])

        prompt_template = f"""
        You are a helpful assistant. Answer the following question based ONLY on the
        provided context. If the context does not contain the answer, state
        that you cannot answer based on the information given. Do not use any
        prior knowledge.

        CONTEXT:
        ---
        {context}
        ---

        QUESTION: {query}

        ANSWER:
        """
        return prompt_template.strip()

    def query(self, query_text: str, n_results: int = 5) -> str:
        """
        Performs a query against the knowledge base.

        Args:
            query_text: The user's question.
            n_results: The number of relevant chunks to retrieve.

        Returns:
            The answer generated by the LLM.
        """
        print(f"Retrieving context for query: '{query_text}'")
        retrieved_docs = self.retriever.retrieve(query_text, n_results=n_results)

        if not retrieved_docs:
            return "I could not find any relevant information in the provided documents to answer your question."

        prompt = self._create_prompt(query_text, retrieved_docs)

        print("Generating answer from LLM...")
        answer = self.llm.generate(prompt)

        return answer
