import os
from typing import List, Literal, Optional, cast

from langchain.docstore.document import Document

from .embedding_manager import EmbeddingManager
from .llm_interface import LLMInterface, JarvisPlatform_LLM
from .retriever import ChromaRetriever
from jarvis.jarvis_utils.config import (
    get_rag_embedding_mode,
    get_rag_vector_db_path,
    get_rag_embedding_cache_path,
    get_rag_embedding_models,
)


class JarvisRAGPipeline:
    """
    The main orchestrator for the RAG pipeline.

    This class integrates the embedding manager, retriever, and LLM to provide
    a complete pipeline for adding documents and querying them.
    """

    def __init__(
        self,
        llm: Optional[LLMInterface] = None,
        embedding_mode: Optional[Literal["performance", "accuracy"]] = None,
        db_path: Optional[str] = None,
        collection_name: str = "jarvis_rag_collection",
    ):
        """
        Initializes the RAG pipeline.

        Args:
            llm: An instance of a class implementing LLMInterface.
                 If None, defaults to OpenAI_LLM().
            embedding_mode: The mode for the local embedding model. If None, uses config value.
            db_path: Path to the persistent vector database. If None, uses config value.
            collection_name: Name of the collection in the vector database.
        """
        # Determine the embedding model to isolate data paths
        _embedding_mode = embedding_mode or get_rag_embedding_mode()
        embedding_models = get_rag_embedding_models()
        model_name = embedding_models[_embedding_mode]["model_name"]
        sanitized_model_name = model_name.replace("/", "_").replace("\\", "_")

        # If a specific db_path is given, use it. Otherwise, create a model-specific path.
        _final_db_path = (
            str(db_path)
            if db_path
            else os.path.join(get_rag_vector_db_path(), sanitized_model_name)
        )
        # Always create a model-specific cache path.
        _final_cache_path = os.path.join(
            get_rag_embedding_cache_path(), sanitized_model_name
        )

        self.embedding_manager = EmbeddingManager(
            mode=cast(Literal["performance", "accuracy"], _embedding_mode),
            cache_dir=_final_cache_path,
        )
        self.retriever = ChromaRetriever(
            embedding_manager=self.embedding_manager,
            db_path=_final_db_path,
            collection_name=collection_name,
        )
        # Default to the local Jarvis Platform LLM
        self.llm = llm if llm is not None else JarvisPlatform_LLM()

        print("✅ JarvisRAGPipeline 初始化成功。")

    def add_documents(self, documents: List[Document]):
        """
        Adds documents to the vector knowledge base.

        Args:
            documents: A list of LangChain Document objects to add.
        """
        self.retriever.add_documents(documents)

    def _create_prompt(self, query: str, context_docs: List[Document]) -> str:
        """Creates the final prompt for the LLM."""
        context = "\n\n".join([doc.page_content for doc in context_docs])

        prompt_template = f"""
        你是一个乐于助人的助手。请仅根据提供的上下文回答以下问题。
        如果上下文中不包含答案，请说明你无法根据给定信息回答。不要使用任何先验知识。

        上下文:
        ---
        {context}
        ---

        问题: {query}

        回答:
        """
        return prompt_template.strip()

    def query(self, query_text: str, n_results: int = 5) -> str:
        """
        Performs a query against the knowledge base.

        Args:
            query_text: The user's question.
            n_results: The number of relevant chunks to retrieve.

        Returns:
            The answer generated by the LLM.
        """
        print(f"🔍 正在为查询检索上下文: '{query_text}'")
        retrieved_docs = self.retriever.retrieve(query_text, n_results=n_results)

        if not retrieved_docs:
            return "我在提供的文档中找不到任何相关信息来回答您的问题。"

        # Print the sources of the retrieved documents
        sources = sorted(
            list(
                {
                    doc.metadata["source"]
                    for doc in retrieved_docs
                    if "source" in doc.metadata
                }
            )
        )
        if sources:
            print(f"📚 根据以下文档回答:")
            for source in sources:
                print(f"  - {source}")

        prompt = self._create_prompt(query_text, retrieved_docs)

        print("🤖 正在从LLM生成答案...")
        answer = self.llm.generate(prompt)

        return answer
