"""
Vector database implementation using FAISS.

This module provides a vector database for efficient similarity search,
specifically designed to work with code embeddings generated by the Qodo-Embed-1-7B model.
"""

import os
import json
import pickle
from httpx import get
import numpy as np
import hashlib
from typing import List, Dict, Any, Tuple, Optional, Union, cast
import functools
import torch
from pathlib import Path
import time
import subprocess

from jarvis.jarvis_utils.output import PrettyOutput, OutputType
from jarvis.jarvis_utils.config import get_code_embeding_model_dimension, get_code_embeding_model_name, get_data_dir
from jarvis.jarvis_utils.code_embeding import embed_file, CodeEmbedding
from jarvis.jarvis_utils.utils import init_env

# 检查是否可以使用faiss
_HAS_FAISS = False
try:
    import importlib.util
    if importlib.util.find_spec("faiss") is not None:
        import faiss  # type: ignore
        _HAS_FAISS = True
        PrettyOutput.print("FAISS库已加载，将使用高效向量搜索", OutputType.DEBUG)
except ImportError:
    PrettyOutput.print("FAISS库未安装或无法加载，请安装FAISS: pip install faiss-cpu 或 pip install faiss-gpu", OutputType.WARNING)


class EmbeddingCache:
    """文件嵌入向量的缓存，避免重复计算"""
    
    def __init__(self, cache_dir: Optional[str] = None):
        """
        初始化嵌入缓存。
        
        Args:
            cache_dir: 缓存目录，如果为None则使用默认目录
        """
        # 如果没有指定路径，使用默认路径
        if cache_dir is None:
            self.cache_dir = os.path.join(get_data_dir(), "embedding_cache")
        else:
            self.cache_dir = cache_dir
            
        # 确保目录存在
        os.makedirs(self.cache_dir, exist_ok=True)
        
        # 缓存索引文件
        self.index_path = os.path.join(self.cache_dir, "cache_index.json")
        self.cache_index: Dict[str, Dict[str, Any]] = {}
        
        # 加载缓存索引
        self._load_index()
    
    def _load_index(self) -> None:
        """加载缓存索引文件"""
        if os.path.exists(self.index_path):
            try:
                with open(self.index_path, 'r', encoding='utf-8') as f:
                    self.cache_index = json.load(f)
                PrettyOutput.print(f"已加载嵌入缓存索引，共 {len(self.cache_index)} 条记录", OutputType.DEBUG)
            except Exception as e:
                PrettyOutput.print(f"加载嵌入缓存索引失败: {str(e)}", OutputType.WARNING)
                self.cache_index = {}
    
    def _save_index(self) -> None:
        """保存缓存索引文件"""
        try:
            with open(self.index_path, 'w', encoding='utf-8') as f:
                json.dump(self.cache_index, f, ensure_ascii=False, indent=2)
        except Exception as e:
            PrettyOutput.print(f"保存嵌入缓存索引失败: {str(e)}", OutputType.WARNING)
    
    def _get_file_hash(self, file_path: str) -> str:
        """
        计算文件的哈希值。
        
        Args:
            file_path: 文件路径
            
        Returns:
            str: 文件的MD5哈希值
        """
        try:
            hasher = hashlib.md5()
            with open(file_path, 'rb') as f:
                # 读取文件内容并计算哈希值
                buffer = f.read(65536)  # 64kb chunks
                while len(buffer) > 0:
                    hasher.update(buffer)
                    buffer = f.read(65536)
            return hasher.hexdigest()
        except Exception as e:
            PrettyOutput.print(f"计算文件哈希值失败: {str(e)}", OutputType.WARNING)
            # 如果计算哈希失败，使用文件路径和修改时间作为备用
            try:
                return f"{file_path}_{os.path.getmtime(file_path)}"
            except:
                return file_path  # 最后的备用选项
    
    def _get_cache_path(self, file_hash: str, model_name: str) -> str:
        """
        获取缓存文件路径。
        
        Args:
            file_hash: 文件哈希值
            model_name: 模型名称
            
        Returns:
            str: 缓存文件路径
        """
        # 使用模型名称的哈希作为子目录，避免路径过长
        model_hash = hashlib.md5(model_name.encode()).hexdigest()[:8]
        cache_dir = os.path.join(self.cache_dir, model_hash)
        os.makedirs(cache_dir, exist_ok=True)
        
        return os.path.join(cache_dir, f"{file_hash}.pkl")
    
    def get(self, file_path: str, model_name: str) -> Optional[List[torch.Tensor]]:
        """
        从缓存中获取文件的嵌入向量。
        
        Args:
            file_path: 文件路径
            model_name: 模型名称
            
        Returns:
            Optional[List[torch.Tensor]]: 文件的嵌入向量，如果缓存未命中则返回None
        """
        if not os.path.exists(file_path):
            return None
        
        # 计算文件哈希
        file_hash = self._get_file_hash(file_path)
        
        # 检查缓存索引
        cache_key = f"{file_hash}_{model_name}"
        cache_info = self.cache_index.get(cache_key)
        
        # 如果没有缓存记录或文件已修改，返回None
        if cache_info is None:
            return None
            
        # 检查文件是否已修改
        file_mtime = os.path.getmtime(file_path)
        if file_mtime > cache_info.get("mtime", 0):
            return None
        
        # 获取缓存文件路径
        cache_path = self._get_cache_path(file_hash, model_name)
        
        # 如果缓存文件不存在，返回None
        if not os.path.exists(cache_path):
            return None
        
        # 加载缓存文件
        try:
            with open(cache_path, 'rb') as f:
                embeddings = pickle.load(f)
            PrettyOutput.print(f"从缓存加载文件 {file_path} 的嵌入向量", OutputType.DEBUG)
            return embeddings
        except Exception as e:
            PrettyOutput.print(f"加载嵌入缓存失败: {str(e)}", OutputType.WARNING)
            return None
    
    def put(self, file_path: str, model_name: str, embeddings: List[torch.Tensor]) -> None:
        """
        将文件的嵌入向量存入缓存。
        
        Args:
            file_path: 文件路径
            model_name: 模型名称
            embeddings: 文件的嵌入向量
        """
        if not os.path.exists(file_path):
            return
        
        # 计算文件哈希
        file_hash = self._get_file_hash(file_path)
        
        # 获取缓存文件路径
        cache_path = self._get_cache_path(file_hash, model_name)
        
        # 保存嵌入向量
        try:
            with open(cache_path, 'wb') as f:
                pickle.dump(embeddings, f)
            
            # 更新缓存索引
            cache_key = f"{file_hash}_{model_name}"
            self.cache_index[cache_key] = {
                "file_path": file_path,
                "model_name": model_name,
                "mtime": os.path.getmtime(file_path),
                "cache_path": cache_path,
                "cache_time": time.time(),
                "embedding_count": len(embeddings)
            }
            
            # 保存缓存索引
            self._save_index()
            
            PrettyOutput.print(f"已缓存文件 {file_path} 的嵌入向量", OutputType.DEBUG)
        except Exception as e:
            PrettyOutput.print(f"保存嵌入缓存失败: {str(e)}", OutputType.WARNING)


# 创建单例缓存实例
_embedding_cache: Optional[EmbeddingCache] = None

def get_embedding_cache() -> EmbeddingCache:
    """
    获取嵌入缓存单例实例。
    
    Returns:
        EmbeddingCache: 嵌入缓存实例
    """
    global _embedding_cache
    if _embedding_cache is None:
        _embedding_cache = EmbeddingCache()
    return _embedding_cache


class CodeVectorDB:
    """A vector database for code embeddings using FAISS."""
    
    def __init__(self, dimension: int, db_path: Optional[str] = None):
        """
        Initialize the vector database.
        
        Args:
            db_path: Path to the database directory. If None, uses default path.
            dimension: Dimension of the embedding vectors (default: 3584 for Qodo-Embed-1-7B)
        """
        if not _HAS_FAISS:
            raise ImportError("FAISS is required for vector database. Please install it with: pip install faiss-cpu")
        
        self.dimension = dimension  # Qodo-Embed-1-7B 默认维度
        
        # 如果没有指定路径，使用默认路径
        if db_path is None:
            self.db_path = os.path.join(get_data_dir(), "vector_db")
        else:
            self.db_path = db_path
            
        # 确保目录存在
        os.makedirs(self.db_path, exist_ok=True)
        
        # 索引文件和元数据文件路径
        self.index_path = os.path.join(self.db_path, "faiss_index.bin")
        self.metadata_path = os.path.join(self.db_path, "metadata.json")
        
        # 初始化或加载索引
        self.index: Any = None  # FAISS索引类型会根据具体使用而定
        self.metadata: Dict[int, Dict[str, Any]] = {}
        self._next_id = 0
        
        # 获取嵌入缓存
        self.embedding_cache = get_embedding_cache()
        
        # 如果文件存在，尝试加载
        self._load_or_create_db()
    
    def _load_or_create_db(self) -> None:
        """Load existing database or create a new one if not exists."""
        if os.path.exists(self.index_path) and os.path.exists(self.metadata_path):
            try:
                PrettyOutput.print(f"尝试加载现有向量数据库: {self.index_path}", OutputType.INFO)
                self.index = faiss.read_index(self.index_path)
                
                with open(self.metadata_path, 'r', encoding='utf-8') as f:
                    self.metadata = json.load(f)
                    # 将字符串键转换为整数键
                    self.metadata = {int(k): v for k, v in self.metadata.items()}
                
                # 计算下一个可用ID
                if self.metadata:
                    self._next_id = max(map(int, self.metadata.keys())) + 1
                else:
                    self._next_id = 0
                
                PrettyOutput.print(f"成功加载向量数据库，共 {len(self.metadata)} 条记录", OutputType.INFO)
            except Exception as e:
                PrettyOutput.print(f"加载向量数据库失败: {str(e)}，将创建新数据库", OutputType.WARNING)
                self._create_new_db()
        else:
            PrettyOutput.print("向量数据库不存在，将创建新数据库", OutputType.INFO)
            self._create_new_db()
    
    def _create_new_db(self) -> None:
        """Create a new FAISS index."""
        # 创建一个新的FAISS索引
        # L2距离度量 - 欧几里得距离
        self.index = faiss.IndexFlatL2(self.dimension)
        self.metadata = {}
        self._next_id = 0
        PrettyOutput.print(f"创建了新的向量数据库，维度: {self.dimension}", OutputType.INFO)
    
    def save(self) -> None:
        """Save the database to disk."""
        if self.index is None:
            PrettyOutput.print("无法保存向量数据库：索引为空", OutputType.ERROR)
            return
        
        try:
            # 保存FAISS索引
            faiss.write_index(self.index, self.index_path)
            
            # 保存元数据
            with open(self.metadata_path, 'w', encoding='utf-8') as f:
                # 确保键是字符串
                serializable_metadata = {str(k): v for k, v in self.metadata.items()}
                json.dump(serializable_metadata, f, ensure_ascii=False, indent=2)
            
            PrettyOutput.print(f"向量数据库已保存到 {self.db_path}", OutputType.INFO)
        except Exception as e:
            PrettyOutput.print(f"保存向量数据库失败: {str(e)}", OutputType.ERROR)
    
    def add_embedding(self, embedding: Union[torch.Tensor, np.ndarray], metadata: Dict[str, Any]) -> int:
        """
        Add a single embedding to the database.
        
        Args:
            embedding: The embedding vector to add
            metadata: Metadata associated with this embedding
            
        Returns:
            int: The ID of the added vector
        """
        if self.index is None:
            raise ValueError("数据库未初始化")
        
        # 确保embedding是numpy数组且形状正确
        if isinstance(embedding, torch.Tensor):
            embedding_np = embedding.detach().cpu().numpy()
        else:
            embedding_np = embedding
        
        # 确保是2D数组
        if len(embedding_np.shape) == 1:
            embedding_np = embedding_np.reshape(1, -1)
        
        # 确保数组是连续的内存布局
        embedding_np = np.ascontiguousarray(embedding_np, dtype=np.float32)
        
        # 记录当前ID
        vector_id = self._next_id
        
        # 添加向量到索引
        self.index.add(embedding_np)
        
        # 添加元数据
        self.metadata[vector_id] = metadata.copy()
        
        # 更新下一个ID
        self._next_id += 1
        
        return vector_id
    
    def add_embeddings(self, embeddings: List[torch.Tensor], metadatas: List[Dict[str, Any]]) -> List[int]:
        """
        Add multiple embeddings to the database.
        
        Args:
            embeddings: List of embedding vectors to add
            metadatas: List of metadata dicts, one for each embedding
            
        Returns:
            List[int]: List of IDs for the added vectors
        """
        if len(embeddings) != len(metadatas):
            raise ValueError(f"嵌入向量数量 ({len(embeddings)}) 与元数据数量 ({len(metadatas)}) 不匹配")
        
        ids = []
        for embedding, metadata in zip(embeddings, metadatas):
            vector_id = self.add_embedding(embedding, metadata)
            ids.append(vector_id)
        
        return ids
    
    def add_file(self, file_path: str, model_name: str, dimension: int) -> List[int]:
        """
        Add a file to the database by generating its embeddings.
        
        Args:
            file_path: Path to the file to be added
            model_name: Name of the model to use for embedding
            
        Returns:
            List[int]: List of IDs for the added vectors
        """
        try:
            file_path = os.path.abspath(file_path)
            
            # 首先检查缓存
            cached_embeddings = self.embedding_cache.get(file_path, model_name)
            
            if cached_embeddings is not None:
                PrettyOutput.print(f"使用缓存的嵌入向量: {file_path}", OutputType.INFO)
                embeddings = cached_embeddings
            else:
                # 没有缓存，生成文件的嵌入向量
                PrettyOutput.print(f"生成文件的嵌入向量: {file_path}", OutputType.INFO)
                embeddings = embed_file(file_path, model_name, dimension)
                
                # 缓存嵌入向量
                self.embedding_cache.put(file_path, model_name, embeddings)
            
            # 准备元数据
            file_size = os.path.getsize(file_path)
            file_mtime = os.path.getmtime(file_path)
            
            # 为文件的每个块创建元数据
            metadatas = []
            for i in range(len(embeddings)):
                metadata = {
                    "file_path": file_path,
                    "file_size": file_size,
                    "file_mtime": file_mtime,
                    "chunk_index": i,
                    "total_chunks": len(embeddings),
                }
                metadatas.append(metadata)
            
            # 添加到数据库
            ids = self.add_embeddings(embeddings, metadatas)
            PrettyOutput.print(f"已将文件 {file_path} 添加到向量数据库，共 {len(ids)} 个嵌入向量", OutputType.INFO)
            
            return ids
        except Exception as e:
            PrettyOutput.print(f"将文件添加到向量数据库失败: {str(e)}", OutputType.ERROR)
            raise
    
    def search(self, query_embedding: Union[torch.Tensor, np.ndarray], top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Search for similar vectors in the database.
        
        Args:
            query_embedding: The query embedding vector
            top_k: Number of results to return
            
        Returns:
            List[Dict[str, Any]]: List of results with metadata and scores
        """
        if self.index is None:
            raise ValueError("数据库未初始化")
        
        if len(self.metadata) == 0:
            PrettyOutput.print("向量数据库为空，无法搜索", OutputType.WARNING)
            return []
        
        # 确保query_embedding是numpy数组且形状正确
        if isinstance(query_embedding, torch.Tensor):
            query_np = query_embedding.detach().cpu().numpy()
        else:
            query_np = query_embedding
        
        # 确保是2D数组
        if len(query_np.shape) == 1:
            query_np = query_np.reshape(1, -1)
        
        # 确保数组是连续的内存布局
        query_np = np.ascontiguousarray(query_np, dtype=np.float32)
        
        # 确保top_k不超过数据库中的向量数量
        actual_top_k = min(top_k, self.index.ntotal)
        if actual_top_k < top_k:
            PrettyOutput.print(f"请求的结果数量 ({top_k}) 超过了数据库中的向量数量 ({self.index.ntotal})，将返回 {actual_top_k} 个结果", OutputType.WARNING)
        
        # 执行搜索
        distances, indices = self.index.search(query_np, actual_top_k)
        
        # 准备结果
        results = []
        for i in range(len(indices[0])):
            idx = indices[0][i]
            distance = distances[0][i]
            
            # 索引可能超出范围，因为FAISS可能返回-1表示没有足够的匹配
            if idx < 0 or idx >= self._next_id:
                continue
            
            # 获取元数据
            metadata = self.metadata.get(int(idx), {})
            
            # 创建结果
            result = {
                "id": int(idx),
                "distance": float(distance),
                "score": float(1.0 / (1.0 + float(distance))),  # 归一化得分
                "metadata": metadata
            }
            results.append(result)
        
        return results
    
    def search_by_text(self, query_text: str, model_name: str, dimension: int, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Search the database using a text query.
        
        Args:
            query_text: The text query
            top_k: Number of results to return
            model_name: Name of the model to use for embedding
            
        Returns:
            List[Dict[str, Any]]: List of results with metadata and scores
        """
        # 创建临时CodeEmbedding实例
        embedder = CodeEmbedding(model_name, dimension)
        
        # 生成查询文本的嵌入向量
        query_embeddings = embedder.embed_code(query_text)
        
        # 使用第一个嵌入向量进行搜索
        if query_embeddings:
            return self.search(query_embeddings[0], top_k)
        else:
            PrettyOutput.print("无法为查询文本生成嵌入向量", OutputType.ERROR)
            return []
    
    def search_by_file(self, file_path: str, model_name: str, dimension: int, top_k: int = 5) -> List[Dict[str, Any]]:
        """
        Search the database using a file as query.
        
        Args:
            file_path: Path to the query file
            top_k: Number of results to return
            model_name: Name of the model to use for embedding
            
        Returns:
            List[Dict[str, Any]]: List of results with metadata and scores
        """
        # 生成文件的嵌入向量
        query_embeddings = embed_file(file_path, model_name, dimension)
        
        # 使用第一个嵌入向量进行搜索
        if query_embeddings:
            return self.search(query_embeddings[0], top_k)
        else:
            PrettyOutput.print(f"无法为文件 {file_path} 生成嵌入向量", OutputType.ERROR)
            return []
    
    def is_text_file(self, file_path: str, sample_size: int = 8192) -> bool:
        """
        检测文件是否为文本文件。
        
        Args:
            file_path: 文件路径
            sample_size: 用于检测的样本大小，默认为8KB
            
        Returns:
            bool: 如果是文本文件返回True，否则返回False
        """
        try:
            # 检查文件是否存在且可读
            if not os.path.isfile(file_path) or not os.access(file_path, os.R_OK):
                return False
                
            # 检查文件是否为空
            if os.path.getsize(file_path) == 0:
                return True  # 空文件视为文本文件
                
            # 读取文件的前面部分
            with open(file_path, 'rb') as f:
                sample = f.read(sample_size)
                
            # 如果样本包含NULL字节，很可能是二进制文件
            if b'\x00' in sample:
                return False
                
            # 尝试将样本解码为文本
            try:
                sample.decode('utf-8')
                return True
            except UnicodeDecodeError:
                # 尝试其他常见编码
                try:
                    sample.decode('iso-8859-1')
                    return True
                except UnicodeDecodeError:
                    pass
                    
            # 统计文本字符比例
            # 如果可打印字符比例超过某个阈值，则认为是文本文件
            printable_chars = sum(1 for c in sample if c >= 32 and c < 127)
            if printable_chars / len(sample) > 0.7:  # 70% 可打印字符
                return True
                
            return False
        except Exception as e:
            PrettyOutput.print(f"检测文件类型时出错 {file_path}: {str(e)}", OutputType.WARNING)
            return False
    
    def _is_file_git_tracked(self, file_path: str) -> bool:
        """
        检查文件是否被Git追踪。

        Args:
            file_path: 文件路径

        Returns:
            bool: 如果文件被Git追踪返回True，否则返回False
        """
        try:
            # 使用git ls-files检查文件是否被追踪
            result = subprocess.run(
                ['git', 'ls-files', '--error-unmatch', file_path],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                check=False  # 不要在命令失败时抛出异常
            )
            # 返回码为0表示文件被Git追踪
            return result.returncode == 0
        except Exception:
            return False

    def _is_in_git_repo(self, directory: str) -> bool:
        """
        检查目录是否在Git仓库中。

        Args:
            directory: 目录路径

        Returns:
            bool: 如果目录在Git仓库中返回True，否则返回False
        """
        try:
            # 获取当前工作目录
            original_dir = os.getcwd()
            
            try:
                # 切换到目标目录
                os.chdir(directory)
                
                # 检查是否在Git仓库中
                result = subprocess.run(
                    ['git', 'rev-parse', '--is-inside-work-tree'],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                    check=False  # 不要在命令失败时抛出异常
                )
                
                # 输出为"true"表示在Git仓库中
                return result.returncode == 0 and result.stdout.strip() == b'true'
            finally:
                # 切回原始目录
                os.chdir(original_dir)
        except Exception:
            return False

    def add_directory(self, directory_path: str,  model_name: str, dimension: int, include_hidden: bool = False, 
                    recursive: bool = True,) -> Dict[str, List[int]]:
        """
        Add all text files in a directory to the database.
        
        Args:
            directory_path: Path to the directory
            include_hidden: Whether to include hidden files (starting with .)
            recursive: Whether to include subdirectories
            model_name: Name of the model to use for embedding
            
        Returns:
            Dict[str, List[int]]: Dictionary mapping file paths to their vector IDs
        """
        if not os.path.isdir(directory_path):
            raise ValueError(f"{directory_path} 不是有效的目录")
        
        # 检查是否在Git仓库中
        is_git_directory = self._is_in_git_repo(directory_path)
        if is_git_directory:
            PrettyOutput.print(f"检测到Git仓库，将只处理Git追踪的文件", OutputType.INFO)
        
        # 收集所有文件
        files_to_process = []
        skipped_binary_files = 0
        skipped_hidden_files = 0
        skipped_non_git_files = 0
        
        def should_process_file(file_path: str) -> bool:
            """判断是否应该处理该文件"""
            # 检查是否隐藏文件
            file_name = os.path.basename(file_path)
            if not include_hidden and file_name.startswith('.'):
                nonlocal skipped_hidden_files
                skipped_hidden_files += 1
                return False
                
            # 如果在Git仓库中，检查文件是否被Git追踪
            if is_git_directory and not self._is_file_git_tracked(file_path):
                nonlocal skipped_non_git_files
                skipped_non_git_files += 1
                return False
                
            # 检查是否文本文件
            if not self.is_text_file(file_path):
                nonlocal skipped_binary_files
                skipped_binary_files += 1
                return False
                
            return True
        
        # 收集文件
        if recursive:
            for root, dirs, files in os.walk(directory_path):
                # 跳过隐藏目录
                if not include_hidden:
                    dirs[:] = [d for d in dirs if not d.startswith('.')]
                
                for file in files:
                    file_path = os.path.join(root, file)
                    if should_process_file(file_path):
                        files_to_process.append(file_path)
        else:
            for file in os.listdir(directory_path):
                file_path = os.path.join(directory_path, file)
                if os.path.isfile(file_path) and should_process_file(file_path):
                    files_to_process.append(file_path)
        
        PrettyOutput.print(f"在目录 {directory_path} 中找到 {len(files_to_process)} 个文本文件要处理", OutputType.INFO)
        if skipped_binary_files > 0:
            PrettyOutput.print(f"跳过了 {skipped_binary_files} 个二进制文件", OutputType.INFO)
        if skipped_hidden_files > 0:
            PrettyOutput.print(f"跳过了 {skipped_hidden_files} 个隐藏文件", OutputType.INFO)
        if skipped_non_git_files > 0:
            PrettyOutput.print(f"跳过了 {skipped_non_git_files} 个非Git追踪文件", OutputType.INFO)
        
        # 处理文件
        results = {}
        skipped_count = 0
        failed_count = 0
        
        for i, file_path in enumerate(files_to_process):
            try:
                # 检查文件是否已在数据库中且未修改
                file_path_abs = os.path.abspath(file_path)
                existing_file = False
                file_modified = True
                
                for metadata in self.metadata.values():
                    if metadata.get("file_path") == file_path_abs:
                        existing_file = True
                        # 检查文件是否已修改
                        if os.path.getmtime(file_path_abs) <= metadata.get("file_mtime", 0):
                            file_modified = False
                            break
                
                if existing_file and not file_modified:
                    PrettyOutput.print(f"跳过未修改的文件 ({i+1}/{len(files_to_process)}): {file_path}", OutputType.INFO)
                    skipped_count += 1
                    
                    # 收集现有文件的ID
                    file_ids = []
                    for vec_id, metadata in self.metadata.items():
                        if metadata.get("file_path") == file_path_abs:
                            file_ids.append(vec_id)
                    
                    results[file_path] = file_ids
                    continue
                
                # 处理文件
                PrettyOutput.print(f"处理文件 ({i+1}/{len(files_to_process)}): {file_path}", OutputType.INFO)
                ids = self.add_file(file_path, model_name, dimension)
                results[file_path] = ids
                
            except Exception as e:
                PrettyOutput.print(f"处理文件 {file_path} 失败: {str(e)}", OutputType.ERROR)
                failed_count += 1
        
        # 保存数据库
        self.save()
        
        PrettyOutput.print(f"目录处理完成。总文本文件数: {len(files_to_process)}, "
                          f"已处理: {len(results) - skipped_count}, "
                          f"已跳过: {skipped_count}, "
                          f"失败: {failed_count}", OutputType.INFO)
        
        return results
    
    def _get_vector_by_id(self, vector_id: int) -> Optional[np.ndarray]:
        """
        Get a vector from the index by its ID.
        
        Note: This is an expensive operation as FAISS doesn't support direct ID-based lookups.
        
        Args:
            vector_id: The ID of the vector to retrieve
            
        Returns:
            Optional[np.ndarray]: The vector if found, None otherwise
        """
        if self.index is None:
            return None
            
        if vector_id not in self.metadata:
            return None
        
        # 检查索引类型并获取向量
        try:
            # FAISS 没有直接从ID获取向量的方法，需要根据索引类型采取不同的方式
            if isinstance(self.index, faiss.IndexFlat) or hasattr(self.index, 'index') and isinstance(getattr(self.index, 'index'), faiss.IndexFlat):
                # 对于IndexFlat，可以直接获取存储的向量
                if hasattr(self.index, 'index'):
                    # IndexIDMap等封装类，获取原始索引
                    flat_index = getattr(self.index, 'index')
                else:
                    flat_index = self.index
                
                # 获取存储的向量
                xb = faiss.vector_to_array(flat_index.xb).reshape(flat_index.ntotal, flat_index.d) # type: ignore
                
                # 确保ID在范围内
                if vector_id < xb.shape[0]:
                    return xb[vector_id]
            else:
                # 对于其他类型的索引，可能无法直接访问向量数据
                PrettyOutput.print(f"无法从索引类型 {type(self.index).__name__} 中直接获取向量", OutputType.WARNING)
        except Exception as e:
            PrettyOutput.print(f"获取向量时出错: {str(e)}", OutputType.ERROR)
        
        return None
    
    def remove_file(self, file_path: str) -> int:
        """
        Remove all vectors associated with a specific file.
        
        Note: This method creates a new index without the specified file's vectors.
        
        Args:
            file_path: Path to the file to remove
            
        Returns:
            int: Number of vectors removed
        """
        if self.index is None:
            raise ValueError("数据库未初始化")
        
        # 规范化文件路径
        file_path = os.path.abspath(file_path)
        
        # 找出要保留的向量ID
        keep_ids = []
        removed_count = 0
        
        # 遍历所有向量
        for vector_id, metadata in self.metadata.items():
            if metadata.get("file_path") != file_path:
                keep_ids.append(vector_id)
            else:
                removed_count += 1
        
        if removed_count == 0:
            PrettyOutput.print(f"数据库中未找到文件 {file_path}，无需删除", OutputType.WARNING)
            return 0
        
        # 如果没有要移除的向量，直接返回
        if len(keep_ids) == len(self.metadata):
            return 0
        
        # 创建一个新的数据库并添加保留的向量
        temp_db_path = os.path.join(self.db_path, "temp")
        os.makedirs(temp_db_path, exist_ok=True)
        
        # 初始化新索引
        new_index = faiss.IndexFlatL2(self.dimension)
        new_metadata = {}
        
        # 从原索引中检索向量并添加到新索引
        for i, vector_id in enumerate(keep_ids):
            # 获取原向量
            vector = self._get_vector_by_id(vector_id)
            if vector is None:
                continue
                
            # 添加到新索引
            vector_reshaped = vector.reshape(1, -1)
            new_index.add(np.ascontiguousarray(vector_reshaped, dtype=np.float32)) # type: ignore
            
            # 更新元数据
            new_metadata[i] = self.metadata[vector_id]
        
        # 替换原索引和元数据
        self.index = new_index
        self.metadata = new_metadata
        self._next_id = len(new_metadata)
        
        # 保存新数据库
        self.save()
        
        PrettyOutput.print(f"已从向量数据库中移除文件 {file_path}，共 {removed_count} 个向量", OutputType.INFO)
        return removed_count
    
    def clear(self) -> None:
        """
        Clear the database by removing all vectors and metadata.
        """
        self._create_new_db()
        self.save()
        PrettyOutput.print("向量数据库已清空", OutputType.INFO)
    
    @property
    def size(self) -> int:
        """
        Get the number of vectors in the database.
        
        Returns:
            int: Number of vectors
        """
        if self.index is None:
            return 0
        return self.index.ntotal


# 创建单例实例
_default_db: Optional[CodeVectorDB] = None

def get_default_db() -> CodeVectorDB:
    """
    获取默认的向量数据库实例。
    
    Returns:
        CodeVectorDB: 默认数据库实例
    """
    global _default_db
    if _default_db is None:
        try:
            _default_db = CodeVectorDB(get_code_embeding_model_dimension())
        except ImportError:
            PrettyOutput.print("无法创建向量数据库: FAISS库未安装。请运行 pip install faiss-cpu", OutputType.ERROR)
            raise
    return _default_db


if __name__ == "__main__":
    import argparse

    init_env()
    
    parser = argparse.ArgumentParser(description="代码向量数据库工具")
    subparsers = parser.add_subparsers(dest="command", help="命令")
    
    # 添加文件命令
    add_parser = subparsers.add_parser("add", help="将文件或目录添加到数据库")
    add_parser.add_argument("path", help="文件或目录路径")
    add_parser.add_argument("--recursive", "-r", action="store_true", help="递归处理目录")
    add_parser.add_argument("--include-hidden", "-i", action="store_true", help="包含隐藏文件和目录")
    
    # 搜索命令
    search_parser = subparsers.add_parser("search", help="搜索数据库")
    search_parser.add_argument("query", help="查询文本或文件路径")
    search_parser.add_argument("--top-k", "-k", type=int, default=5, help="返回结果数量")
    search_parser.add_argument("--is-file", "-f", action="store_true", help="查询是文件路径")
    
    # 删除命令
    remove_parser = subparsers.add_parser("remove", help="从数据库中删除文件")
    remove_parser.add_argument("path", help="文件路径")
    
    # 清空命令
    clear_parser = subparsers.add_parser("clear", help="清空数据库")
    
    # 信息命令
    info_parser = subparsers.add_parser("info", help="显示数据库信息")
    
    args = parser.parse_args()
    
    try:
        db = get_default_db()
        
        if args.command == "add":
            path = os.path.abspath(args.path)
            if os.path.isfile(path):
                ids = db.add_file(path, get_code_embeding_model_name(), get_code_embeding_model_dimension())
                db.save()
                print(f"已将文件 {path} 添加到数据库，ID: {ids}")
            elif os.path.isdir(path):
                results = db.add_directory(path, get_code_embeding_model_name(), get_code_embeding_model_dimension(), include_hidden=args.include_hidden, recursive=args.recursive)
                print(f"已将目录 {path} 中的 {len(results)} 个文件添加到数据库")
            else:
                print(f"路径 {path} 无效")
                
        elif args.command == "search":
            if args.is_file and os.path.isfile(args.query):
                results = db.search_by_file(args.query, get_code_embeding_model_name(), get_code_embeding_model_dimension(), top_k=args.top_k)
            else:
                results = db.search_by_text(args.query, get_code_embeding_model_name(), get_code_embeding_model_dimension(), top_k=args.top_k)
                
            print(f"找到 {len(results)} 个结果:")
            for i, result in enumerate(results):
                metadata = result["metadata"]
                file_path = metadata.get("file_path", "未知")
                chunk_index = metadata.get("chunk_index", 0)
                total_chunks = metadata.get("total_chunks", 1)
                print(f"{i+1}. 文件: {file_path}")
                print(f"   块: {chunk_index+1}/{total_chunks}")
                print(f"   相似度得分: {result['score']:.4f}")
                print()
                
        elif args.command == "remove":
            path = os.path.abspath(args.path)
            removed = db.remove_file(path)
            print(f"已从数据库中删除文件 {path}，移除了 {removed} 个向量")
            
        elif args.command == "clear":
            db.clear()
            print("数据库已清空")
            
        elif args.command == "info":
            size = db.size
            print(f"向量数据库包含 {size} 个向量")
            print(f"向量维度: {db.dimension}")
            print(f"数据库路径: {db.db_path}")
            
        else:
            parser.print_help()
            
    except Exception as e:
        print(f"错误: {str(e)}")
